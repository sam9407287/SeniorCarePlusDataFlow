#!/bin/bash
# åœ¨ GCP Dataflow ä¸Šé‹è¡Œ Pipeline

set -e

# é…ç½®
PROJECT_ID=${1:-senior-care-plus-prod}
REGION=${2:-asia-east1}
BUCKET_NAME=${3:-senior-care-prod-temp}
ENV=${4:-prod}

echo "ğŸš€ éƒ¨ç½²åˆ° Google Cloud Dataflow"
echo "  é …ç›®: $PROJECT_ID"
echo "  å€åŸŸ: $REGION"
echo "  ç’°å¢ƒ: $ENV"

# è¨­ç½® GCP é …ç›®
gcloud config set project $PROJECT_ID

# æª¢æŸ¥ Bucket æ˜¯å¦å­˜åœ¨
if ! gsutil ls gs://$BUCKET_NAME > /dev/null 2>&1; then
    echo "ğŸ“ å»ºç«‹ GCS Bucket..."
    gsutil mb -l $REGION gs://$BUCKET_NAME
fi

# æ§‹å»º Python ç’°å¢ƒ
echo "ğŸ“¦ æº–å‚™ Python ç’°å¢ƒ..."
pip install -r requirements.txt

# é‹è¡Œ Pipeline
echo "ğŸš€ å•Ÿå‹• Dataflow Job..."
python -m src.main \
  --runner DataflowRunner \
  --env $ENV \
  --pipeline both \
  --input-type pubsub \
  --input-topic projects/$PROJECT_ID/topics/gateway-events \
  --output-bigquery $PROJECT_ID:senior_care_analytics.gateway_events \
  --log-level INFO

echo "âœ… Dataflow Job å·²æäº¤"
echo ""
echo "ğŸ“Š ç›£æ§ï¼š"
echo "  https://console.cloud.google.com/dataflow/jobs?project=$PROJECT_ID"

