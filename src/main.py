"""
Main entry point for DataFlow Pipeline
"""

import argparse
import sys
import logging

from src.pipelines.gateway_flattening import GatewayFlatteningPipeline
from src.pipelines.anchor_flattening import AnchorFlatteningPipeline
from src.config import get_config
from src.utils import setup_logger


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    
    parser = argparse.ArgumentParser(
        description="Senior Care Plus DataFlow Pipeline"
    )
    
    # åŸºæœ¬åƒæ•¸
    parser.add_argument(
        "--runner",
        choices=["DirectRunner", "DataflowRunner"],
        default="DirectRunner",
        help="Pipeline åŸ·è¡Œå™¨ (default: DirectRunner)"
    )
    
    parser.add_argument(
        "--env",
        choices=["dev", "test", "prod"],
        default="dev",
        help="ç’°å¢ƒé…ç½® (default: dev)"
    )
    
    parser.add_argument(
        "--pipeline",
        choices=["gateway", "anchor", "both"],
        default="gateway",
        help="è¦åŸ·è¡Œçš„ Pipeline (default: gateway)"
    )
    
    # è¼¸å…¥åƒæ•¸
    parser.add_argument(
        "--input-type",
        choices=["file", "pubsub"],
        default="file",
        help="è¼¸å…¥é¡å‹ (default: file)"
    )
    
    parser.add_argument(
        "--input-file",
        help="è¼¸å…¥æ–‡ä»¶è·¯å¾‘ (file æ¨¡å¼)"
    )
    
    parser.add_argument(
        "--input-topic",
        help="Pub/Sub ä¸»é¡Œ (pubsub æ¨¡å¼)"
    )
    
    # è¼¸å‡ºåƒæ•¸
    parser.add_argument(
        "--output-file",
        help="è¼¸å‡ºæ–‡ä»¶è·¯å¾‘ (ç”¨æ–¼æ¸¬è©¦)"
    )
    
    parser.add_argument(
        "--output-bigquery",
        help="BigQuery è¼¸å‡ºè¡¨ (æ ¼å¼: project:dataset.table)"
    )
    
    # æ—¥èªŒåƒæ•¸
    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="æ—¥èªŒç´šåˆ¥ (default: INFO)"
    )
    
    args = parser.parse_args()
    
    # è¨­ç½®æ—¥èªŒ
    logger = setup_logger("dataflow", args.log_level)
    logger.info(f"å•Ÿå‹• DataFlow Pipeline (ç’°å¢ƒ: {args.env})")
    
    # è®€å–é…ç½®
    config = get_config(args.env)
    logger.info(f"é …ç›®: {config.project_id}, å€åŸŸ: {config.region}")
    
    # é©—è­‰è¼¸å…¥
    if args.input_type == "file" and not args.input_file:
        logger.error("--input-file åƒæ•¸å¿…é ˆæä¾›")
        sys.exit(1)
    
    if args.input_type == "pubsub" and not args.input_topic:
        logger.error("--input-topic åƒæ•¸å¿…é ˆæä¾›")
        sys.exit(1)
    
    # åŸ·è¡Œ Pipeline
    try:
        if args.pipeline in ["gateway", "both"]:
            logger.info("åŸ·è¡Œ Gateway æ‰å¹³åŒ– Pipeline...")
            gateway_pipeline = GatewayFlatteningPipeline(
                project_id=config.project_id,
                region=config.region
            )
            gateway_pipeline.run(
                runner=args.runner,
                input_type=args.input_type,
                input_path=args.input_file,
                input_topic=args.input_topic,
                output_bigquery=args.output_bigquery,
                output_file=args.output_file
            )
            logger.info("âœ… Gateway Pipeline å®Œæˆ")
        
        if args.pipeline in ["anchor", "both"]:
            logger.info("åŸ·è¡Œ Anchor æ‰å¹³åŒ– Pipeline...")
            anchor_pipeline = AnchorFlatteningPipeline(
                project_id=config.project_id,
                region=config.region
            )
            anchor_pipeline.run(
                runner=args.runner,
                input_type=args.input_type,
                input_path=args.input_file,
                input_topic=args.input_topic,
                output_bigquery=args.output_bigquery,
                output_file=args.output_file
            )
            logger.info("âœ… Anchor Pipeline å®Œæˆ")
        
        logger.info("ğŸ‰ æ‰€æœ‰ Pipeline åŸ·è¡Œå®Œæˆ")
        return 0
        
    except Exception as e:
        logger.error(f"Pipeline åŸ·è¡Œå¤±æ•—: {str(e)}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())

